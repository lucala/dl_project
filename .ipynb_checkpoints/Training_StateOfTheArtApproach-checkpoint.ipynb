{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PrepareOriginalData import PrepareData\n",
    "import numpy as np\n",
    "\n",
    "# Some constants\n",
    "taskType = 'OpenEnded'\n",
    "data_amount = 10\n",
    "epochs = 50\n",
    "\n",
    "# Train on only one question type\n",
    "question_type = 'all'\n",
    "\n",
    "#use 6 and 3 for questions and answers!\n",
    "\n",
    "# Load training set\n",
    "p = PrepareData(path_images='data_vqa_feat', # Path to image features \n",
    "                subset='train2014', # Desired subset: either train2014 or val2014\n",
    "                taskType=taskType, # 'OpenEnded', 'MultipleChoice', 'all'\n",
    "                cut_data=data_amount, # Percentage of data to use, 1 = All values, above 1=#samples for debugging\n",
    "                output_path='data', # Path where we want to output temporary data\n",
    "                pad_length=14, # Number of words in a question (zero padded)\n",
    "                question_threshold=6, answer_threshold=3, # Keep only most common words #yes/no use 10\n",
    "                answers_sparse=True, questions_sparse=True, answer_type=question_type)\n",
    "image_features, questions, answers, annotations = p.load_data()\n",
    "print(\"Image features\", image_features.shape)\n",
    "print(\"Question features\", questions.shape)\n",
    "print(\"Answers\", answers.shape)\n",
    "print(\"Dictionary size\", p.dic_size)\n",
    "print(\"Number of possible classes\", np.max(answers) + 1)\n",
    "\n",
    "# Save dictionary\n",
    "p.dumpDictionary('dictionary_other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Glove embedding\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('data/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((np.max(questions) + 1, 100)) # Ignore thresholded words\n",
    "for word in p._question_dict.keys():\n",
    "    # If word was thresholded ignore it\n",
    "    if p._question_dict[word] != 0:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[p._question_dict[word]] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 100, 100)     3200        word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 512)          941568      word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 16, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 16, 512)      0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 1536)     0           repeat_vector_2[0][0]            \n",
      "                                                                 image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 16, 512)      786944      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16, 512)      786944      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 16, 512)      0           dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 16, 1)        513         multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 16)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 16)           272         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1024)         0           image_input[0][0]                \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          262656      gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          262656      gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 512)          524800      dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 512)          524800      dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 512)          0           dense_21[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 512)          0           dense_19[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 512)          0           multiply_9[0][0]                 \n",
      "                                                                 multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 100)          51300       multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 100)          51300       multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1024)         525312      multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1024)         525312      multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 100)          0           dense_23[0][0]                   \n",
      "                                                                 dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 1024)         0           dense_26[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 101)          10201       multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 101)          103525      multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 101)          0           dense_25[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 101)          10302       add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,371,605\n",
      "Trainable params: 5,368,405\n",
      "Non-trainable params: 3,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from NeuralNetworkStateOfTheArtApproach import NeuralNetwork\n",
    "# Use this when using sparse representation\n",
    "#neuralnet = NeuralNetwork(image_features.shape[0],1024,questions.shape[1],p.dic_size, embedding_matrix, np.max(answers)+1, epochs = epochs, batchSize=256)\n",
    "\n",
    "import numpy as np\n",
    "image_features = np.random.rand(10,16,1024) #10 samples, 16 locations, 1000 dimensions\n",
    "questions = np.random.rand(10,100) #10 samples, 100 dimensions (embedding)\n",
    "answers = np.random.rand(10,100) #10 samples, 100 dimensions (answer classes)\n",
    "embedding_matrix = np.random.rand(32,100)\n",
    "\n",
    "neuralnet = NeuralNetwork(image_features.shape[0],1024,questions.shape[1], 32, embedding_matrix, np.max(answers)+1, epochs = 5, batchSize=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 16, 1024) (10, 100) (10, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected output_layer to have shape (None, 1) but got array with shape (10, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-500f118dd973>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\deepLearningRepo\\NeuralNetworkStateOfTheArtApproach.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, images, questions, labels)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, reduceLR])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lanze\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1581\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1582\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lanze\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1419\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1420\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mC:\\Users\\lanze\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    151\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected output_layer to have shape (None, 1) but got array with shape (10, 100)"
     ]
    }
   ],
   "source": [
    "# Train network\n",
    "neuralnet.fit(image_features, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.93012561e-10   6.84771478e-17   9.29371022e-19 ...,   5.97085767e-18\n",
      "    9.05393538e-19   3.94834632e-19]\n",
      " [  1.92761448e-07   1.60113926e-20   1.89712921e-16 ...,   6.78813895e-21\n",
      "    1.29941199e-17   8.65425761e-20]\n",
      " [  3.75048863e-03   1.02968111e-11   1.58632699e-12 ...,   7.82832085e-16\n",
      "    7.07759914e-13   9.44638333e-15]\n",
      " ..., \n",
      " [  2.08269989e-06   5.18023802e-14   5.24298080e-14 ...,   2.30640878e-16\n",
      "    2.06009932e-15   5.26853501e-17]\n",
      " [  7.22859511e-07   5.54704259e-14   4.56072810e-12 ...,   1.47832634e-16\n",
      "    3.01057600e-14   1.21976937e-15]\n",
      " [  3.33514833e-03   4.09302573e-16   1.34520563e-12 ...,   5.16170197e-11\n",
      "    2.27623672e-15   1.73830600e-17]]\n",
      "[[1600]\n",
      " [  13]\n",
      " [5595]\n",
      " ..., \n",
      " [ 912]\n",
      " [ 504]\n",
      " [3524]]\n"
     ]
    }
   ],
   "source": [
    "#see what it predicts on training data, on which it trained!\n",
    "predTrain = neuralnet.predict_current_state(image_features, questions)\n",
    "print(predTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:02.492628\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:04.294313\n",
      "creating index...\n",
      "index created!\n",
      "Image features (124216, 1024)\n",
      "Question features (124216, 32)\n",
      "Dictionary size 8994\n"
     ]
    }
   ],
   "source": [
    "image_features = questions = answers = annotations = []\n",
    "question_type = 'other'\n",
    "# Load validation set and evaluate prediction on it\n",
    "pt= PrepareData(path_images='data_vqa_feat', # Path to image features \n",
    "                        subset='val2014', # Desired subset: either train2014 or val2014\n",
    "                        taskType='all', # 'OpenEnded', 'MultipleChoice', 'all'\n",
    "                        cut_data=data_amount, # Percentage of data to use, 1 = All values, above 1 = 10 samples for debugging\n",
    "                        output_path='data', # Path where we want to output temporary data\n",
    "                        pad_length=32, # Number of words in a question (zero padded)\n",
    "                        question_threshold=6, answer_threshold=3, # Keep only most common words\n",
    "                        answers_sparse=True, questions_sparse=True, answer_type=question_type,\n",
    "                        precomputed_dic=p._question_dict)\n",
    "pt.loadDictionary('data/dictionary_other.pkl') # Use same dictionary as in training\n",
    "image_features, questions, answers, annotations = pt.load_data()\n",
    "print(\"Image features\", image_features.shape)\n",
    "print(\"Question features\", questions.shape)\n",
    "print(\"Dictionary size\", pt.dic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.39707490e-05,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  9.87900496e-01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.46565493e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  2.71191776e-01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.26154744e-03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.79042786e-01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "pred = neuralnet.predict_current_state(image_features, object_matrix, questions)# 'weights/weights-60-0.5559.hdf5')\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#average over multiple models\n",
    "pred1 = pred2 = pred3 = pred4 = pred5 = pred6 = pred = []\n",
    "pred1 = neuralnet.predict(image_features, questions, 'weights/weights-99-0.4507.hdf5')\n",
    "pred1 += neuralnet.predict(image_features, questions, 'weights/weights-91-0.5499.hdf5')\n",
    "pred1 += neuralnet.predict(image_features, questions, 'weights/weights-87-0.5516.hdf5')\n",
    "pred1 += neuralnet.predict(image_features, questions, 'weights/weights-60-0.5559Other60epochs.hdf5')\n",
    "pred1 += neuralnet.predict(image_features, questions, 'weights/weights-58-0.5690.hdf5')\n",
    "pred1 += neuralnet.predict(image_features, questions, 'weights/weights-56-0.5697.hdf5')\n",
    "pred1 /= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124508, 5761)\n"
     ]
    }
   ],
   "source": [
    "#pred = (pred1+pred2+pred3+pred4+pred5+pred6)/6\n",
    "#print(pred1.shape)\n",
    "#pred = numpy.round(pred)+1\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer type classification accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lanze\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:177: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  score = y_true == y_pred\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# TODO: can probably still improve this accuracy\n",
    "print('Answer type classification accuracy:', accuracy_score(pred, answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:02.561160\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.14s)\n",
      "creating index...\n",
      "index created!\n",
      "computing accuracy\n",
      "Finshed Percent: [####################] 99% Done computing accuracy\n",
      "\n",
      "\n",
      "Overall Accuracy is: 15.32\n",
      "\n",
      "Per Question Type Accuracy is the following:\n",
      "what is the : 9.70\n",
      "what : 13.23\n",
      "what is in the : 7.52\n",
      "what is : 7.32\n",
      "what color is the : 30.19\n",
      "how : 1.73\n",
      "why : 1.86\n",
      "none of the above : 6.86\n",
      "what is the person : 20.54\n",
      "what are : 13.12\n",
      "who is : 9.66\n",
      "what brand : 9.52\n",
      "what kind of : 14.14\n",
      "what type of : 12.56\n",
      "is the : 6.34\n",
      "is : 2.37\n",
      "what is the man : 19.36\n",
      "what is on the : 6.42\n",
      "what color are the : 28.95\n",
      "where is the : 6.55\n",
      "what time : 10.59\n",
      "does the : 3.07\n",
      "does this : 0.71\n",
      "is the person : 6.88\n",
      "is it : 9.57\n",
      "what room is : 71.78\n",
      "what sport is : 70.63\n",
      "which : 21.01\n",
      "what does the : 6.14\n",
      "what color : 21.80\n",
      "is this : 2.47\n",
      "what is the name : 0.74\n",
      "where are the : 7.42\n",
      "was : 2.27\n",
      "what animal is : 38.88\n",
      "what is the color of the : 28.72\n",
      "what is the woman : 12.07\n",
      "what are the : 9.61\n",
      "are these : 0.00\n",
      "what is this : 20.86\n",
      "do : 8.52\n",
      "is that a : 1.30\n",
      "what color is : 29.13\n",
      "are the : 2.11\n",
      "is this a : 5.70\n",
      "why is the : 2.22\n",
      "how many : 0.00\n",
      "is this person : 0.00\n",
      "is there : 0.00\n",
      "is the man : 6.23\n",
      "are : 0.00\n",
      "is this an : 0.00\n",
      "are there : 0.00\n",
      "is the woman : 4.06\n",
      "are they : 2.78\n",
      "can you : 0.00\n",
      "is he : 9.58\n",
      "has : 0.00\n",
      "is there a : 9.09\n",
      "what number is : 0.00\n",
      "do you : 5.00\n",
      "how many people are : 0.00\n",
      "\n",
      "\n",
      "Per Answer Type Accuracy is the following:\n",
      "other : 15.65\n",
      "number : 0.78\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from EvaluateModel import ProduceResult\n",
    "model_evaluator = ProduceResult(p._int_to_answer, p._answer_to_int, dataSubType='val2014')\n",
    "answers = model_evaluator.produce_results(pred, pt._original_questions)\n",
    "model_evaluator.evaluate(taskType=taskType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
