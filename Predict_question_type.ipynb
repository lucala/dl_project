{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:04.023371\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:09.406677\n",
      "creating index...\n",
      "index created!\n",
      "Question features (496698, 32)\n",
      "Dictionary size 14178\n"
     ]
    }
   ],
   "source": [
    "from PrepareOriginalData import PrepareData\n",
    "import numpy as np\n",
    "\n",
    "# Some constants\n",
    "taskType = 'all'\n",
    "data_amount = 1\n",
    "epochs = 50\n",
    "\n",
    "# Load training set\n",
    "p = PrepareData(path_images='data_vqa_feat', # Path to image features \n",
    "                subset='train2014', # Desired subset: either train2014 or val2014\n",
    "                taskType=taskType, # 'OpenEnded', 'MultipleChoice', 'all'\n",
    "                cut_data=data_amount, # Percentage of data to use, 1 = All values, above 1=#samples for debugging\n",
    "                output_path='data', # Path where we want to output temporary data\n",
    "                pad_length=32, # Number of words in a question (zero padded)\n",
    "                question_threshold=0, answer_threshold=0, # Keep only most common words\n",
    "                answers_sparse=True, questions_sparse=True)\n",
    "_, questions, _, annotations = p.load_data()\n",
    "print(\"Question features\", questions.shape)\n",
    "print(\"Dictionary size\", p.dic_size)\n",
    "\n",
    "# Save dictionary\n",
    "p.dumpDictionary('dictionary_all_types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496698,)\n"
     ]
    }
   ],
   "source": [
    "# Get labels\n",
    "y = np.array([2 if ann['answer_type'] == 'number' else 1 if ann['answer_type'] == 'yes/no' else 0 for ann in annotations])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input (InputLayer)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, 32, 64)            907392    \n",
      "_________________________________________________________________\n",
      "flatten_embedding (Flatten)  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 3,705,219\n",
      "Trainable params: 3,705,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#classifier = RandomForestClassifier()\n",
    "#classifier.fit(questions, y)\n",
    "from NeuralNetworkQuestionType import NeuralNetwork\n",
    "neuralnetQuestionType = NeuralNetwork(questions.shape[1], p.dic_size, 3, epochs=50, batchSize=64, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neuralnetQuestionType.fit(questions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:03.904357\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:07.144110\n",
      "creating index...\n",
      "index created!\n",
      "Image features (190604, 1024)\n",
      "Question features (190604, 32)\n",
      "Answers (190604, 1)\n",
      "Dictionary size 14178\n",
      "Number of possible classes 3\n"
     ]
    }
   ],
   "source": [
    "# Train on only one question type\n",
    "question_type = 'yes/no'\n",
    "\n",
    "# Load training set\n",
    "p = PrepareData(path_images='data_vqa_feat', # Path to image features \n",
    "                subset='train2014', # Desired subset: either train2014 or val2014\n",
    "                taskType=taskType, # 'OpenEnded', 'MultipleChoice', 'all'\n",
    "                cut_data=data_amount, # Percentage of data to use, 1 = All values, above 1=#samples for debugging\n",
    "                output_path='data', # Path where we want to output temporary data\n",
    "                pad_length=32, # Number of words in a question (zero padded)\n",
    "                question_threshold=0, answer_threshold=10, # Keep only most common words\n",
    "                answers_sparse=True, questions_sparse=True, answer_type=question_type,\n",
    "                precomputed_dic=p._question_dict)\n",
    "image_features, questions, answers, annotations = p.load_data()\n",
    "print(\"Image features\", image_features.shape)\n",
    "print(\"Question features\", questions.shape)\n",
    "print(\"Answers\", answers.shape)\n",
    "print(\"Dictionary size\", p.dic_size)\n",
    "print(\"Number of possible classes\", np.max(answers) + 1)\n",
    "\n",
    "# Save dictionary\n",
    "p.dumpDictionary('dictionary_yes_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object features\n",
    "from ExtractObjects import ExtractObjects\n",
    "\n",
    "# Consider three thresholds\n",
    "eo = ExtractObjects(cut_data=data_amount, output_fileName='objects_train.txt', subset='train2014', threshold=25)\n",
    "object_matrix1 = eo.onehotvector(annotations)\n",
    "eo = ExtractObjects(cut_data=data_amount, output_fileName='objects_train.txt', subset='train2014', threshold=50)\n",
    "object_matrix2 = eo.onehotvector(annotations)\n",
    "eo = ExtractObjects(cut_data=data_amount, output_fileName='objects_train.txt', subset='train2014', threshold=75)\n",
    "object_matrix3 = eo.onehotvector(annotations)\n",
    "\n",
    "object_matrix = np.concatenate([object_matrix1, object_matrix2, object_matrix3], axis = 1)\n",
    "print(object_matrix.shape)\n",
    "np.save('data/object_matrix_train_yesno.npy', object_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input (InputLayer)         (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 32, 32)       453696      word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_embedding (Flatten)     (None, 1024)         0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2048)         0           image_input[0][0]                \n",
      "                                                                 flatten_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 3)            6147        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 459,843\n",
      "Trainable params: 459,843\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from NeuralNetworkYesNo import NeuralNetwork\n",
    "# Use this when using sparse representation\n",
    "neuralnet = NeuralNetwork(image_features.shape[0],1024,questions.shape[1],p.dic_size,np.max(answers)+1, lr=0.001, objects=240,\n",
    "                          epochs = epochs, batchSize=512, loss='sparse_categorical_crossentropy', activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190604, 1024) (190604, 32) (190604, 1)\n",
      "Train on 133422 samples, validate on 57182 samples\n",
      "Epoch 1/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.7539 - acc: 0.5876Epoch 00001: val_acc improved from -inf to 0.64055, saving model to weights/weights-01-0.6422.hdf5\n",
      "133422/133422 [==============================] - 6s 45us/step - loss: 0.7534 - acc: 0.5878 - val_loss: 0.6422 - val_acc: 0.6406\n",
      "Epoch 2/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.6890Epoch 00002: val_acc improved from 0.64055 to 0.71157, saving model to weights/weights-02-0.5621.hdf5\n",
      "133422/133422 [==============================] - 7s 49us/step - loss: 0.5920 - acc: 0.6891 - val_loss: 0.5621 - val_acc: 0.7116\n",
      "Epoch 3/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.7241Epoch 00003: val_acc improved from 0.71157 to 0.71202, saving model to weights/weights-03-0.5587.hdf5\n",
      "133422/133422 [==============================] - 6s 44us/step - loss: 0.5439 - acc: 0.7240 - val_loss: 0.5587 - val_acc: 0.7120\n",
      "Epoch 4/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.7473Epoch 00004: val_acc improved from 0.71202 to 0.73611, saving model to weights/weights-04-0.5284.hdf5\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.5120 - acc: 0.7473 - val_loss: 0.5284 - val_acc: 0.7361\n",
      "Epoch 5/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.7547Epoch 00005: val_acc improved from 0.73611 to 0.75419, saving model to weights/weights-05-0.4990.hdf5\n",
      "133422/133422 [==============================] - 7s 49us/step - loss: 0.4967 - acc: 0.7545 - val_loss: 0.4990 - val_acc: 0.7542\n",
      "Epoch 6/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.7670Epoch 00006: val_acc improved from 0.75419 to 0.75919, saving model to weights/weights-06-0.4875.hdf5\n",
      "133422/133422 [==============================] - 6s 45us/step - loss: 0.4762 - acc: 0.7670 - val_loss: 0.4875 - val_acc: 0.7592\n",
      "Epoch 7/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.7743Epoch 00007: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 55us/step - loss: 0.4644 - acc: 0.7743 - val_loss: 0.4957 - val_acc: 0.7549\n",
      "Epoch 8/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.7773Epoch 00008: val_acc improved from 0.75919 to 0.76769, saving model to weights/weights-08-0.4712.hdf5\n",
      "133422/133422 [==============================] - 7s 53us/step - loss: 0.4574 - acc: 0.7772 - val_loss: 0.4712 - val_acc: 0.7677\n",
      "Epoch 9/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.7818Epoch 00009: val_acc improved from 0.76769 to 0.77056, saving model to weights/weights-09-0.4652.hdf5\n",
      "133422/133422 [==============================] - 6s 46us/step - loss: 0.4488 - acc: 0.7819 - val_loss: 0.4652 - val_acc: 0.7706\n",
      "Epoch 10/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.7822Epoch 00010: val_acc did not improve\n",
      "133422/133422 [==============================] - 8s 58us/step - loss: 0.4465 - acc: 0.7822 - val_loss: 0.4713 - val_acc: 0.7654\n",
      "Epoch 11/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.7845Epoch 00011: val_acc improved from 0.77056 to 0.77334, saving model to weights/weights-11-0.4568.hdf5\n",
      "133422/133422 [==============================] - 7s 53us/step - loss: 0.4410 - acc: 0.7846 - val_loss: 0.4568 - val_acc: 0.7733\n",
      "Epoch 12/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.7836Epoch 00012: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 52us/step - loss: 0.4407 - acc: 0.7835 - val_loss: 0.4907 - val_acc: 0.7496\n",
      "Epoch 13/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.7893Epoch 00013: val_acc improved from 0.77334 to 0.77528, saving model to weights/weights-13-0.4526.hdf5\n",
      "133422/133422 [==============================] - 6s 48us/step - loss: 0.4319 - acc: 0.7894 - val_loss: 0.4526 - val_acc: 0.7753\n",
      "Epoch 14/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.7909Epoch 00014: val_acc improved from 0.77528 to 0.77566, saving model to weights/weights-14-0.4511.hdf5\n",
      "133422/133422 [==============================] - 8s 58us/step - loss: 0.4281 - acc: 0.7908 - val_loss: 0.4511 - val_acc: 0.7757\n",
      "Epoch 15/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.7918Epoch 00015: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.4258 - acc: 0.7916 - val_loss: 0.4509 - val_acc: 0.7744\n",
      "Epoch 16/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.7899Epoch 00016: val_acc did not improve\n",
      "133422/133422 [==============================] - 8s 64us/step - loss: 0.4288 - acc: 0.7899 - val_loss: 0.4526 - val_acc: 0.7721\n",
      "Epoch 17/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.7934Epoch 00017: val_acc improved from 0.77566 to 0.77841, saving model to weights/weights-17-0.4448.hdf5\n",
      "133422/133422 [==============================] - 7s 56us/step - loss: 0.4222 - acc: 0.7932 - val_loss: 0.4448 - val_acc: 0.7784\n",
      "Epoch 18/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.7942Epoch 00018: val_acc improved from 0.77841 to 0.77890, saving model to weights/weights-18-0.4448.hdf5\n",
      "133422/133422 [==============================] - 7s 53us/step - loss: 0.4203 - acc: 0.7941 - val_loss: 0.4448 - val_acc: 0.7789\n",
      "Epoch 19/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.7941Epoch 00019: val_acc improved from 0.77890 to 0.78019, saving model to weights/weights-19-0.4389.hdf5\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.4188 - acc: 0.7939 - val_loss: 0.4389 - val_acc: 0.7802\n",
      "Epoch 20/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.7964- ETA: 5s - loss: 0.4049 - Epoch 00020: val_acc improved from 0.78019 to 0.78259, saving model to weights/weights-20-0.4356.hdf5\n",
      "133422/133422 [==============================] - 9s 66us/step - loss: 0.4166 - acc: 0.7965 - val_loss: 0.4356 - val_acc: 0.7826\n",
      "Epoch 21/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.7947Epoch 00021: val_acc improved from 0.78259 to 0.78269, saving model to weights/weights-21-0.4370.hdf5\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.4164 - acc: 0.7947 - val_loss: 0.4370 - val_acc: 0.7827\n",
      "Epoch 22/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.7951Epoch 00022: val_acc did not improve\n",
      "133422/133422 [==============================] - 6s 48us/step - loss: 0.4174 - acc: 0.7950 - val_loss: 0.4507 - val_acc: 0.7713\n",
      "Epoch 23/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.7947Epoch 00023: val_acc did not improve\n",
      "133422/133422 [==============================] - 8s 59us/step - loss: 0.4168 - acc: 0.7947 - val_loss: 0.4335 - val_acc: 0.7826\n",
      "Epoch 24/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.7962Epoch 00024: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.4147 - acc: 0.7962 - val_loss: 0.4823 - val_acc: 0.7633\n",
      "Epoch 25/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.7953Epoch 00025: val_acc improved from 0.78269 to 0.78369, saving model to weights/weights-25-0.4313.hdf5\n",
      "133422/133422 [==============================] - 9s 65us/step - loss: 0.4144 - acc: 0.7952 - val_loss: 0.4313 - val_acc: 0.7837\n",
      "Epoch 26/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.7949Epoch 00026: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 55us/step - loss: 0.4147 - acc: 0.7949 - val_loss: 0.4354 - val_acc: 0.7813\n",
      "Epoch 27/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.7967Epoch 00027: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 53us/step - loss: 0.4115 - acc: 0.7966 - val_loss: 0.4312 - val_acc: 0.7826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.7971Epoch 00028: val_acc improved from 0.78369 to 0.78443, saving model to weights/weights-28-0.4299.hdf5\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.4118 - acc: 0.7971 - val_loss: 0.4299 - val_acc: 0.7844\n",
      "Epoch 29/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.7993Epoch 00029: val_acc did not improve\n",
      "133422/133422 [==============================] - 6s 47us/step - loss: 0.4086 - acc: 0.7993 - val_loss: 0.4497 - val_acc: 0.7767\n",
      "Epoch 30/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.7986Epoch 00030: val_acc improved from 0.78443 to 0.78680, saving model to weights/weights-30-0.4277.hdf5\n",
      "133422/133422 [==============================] - 6s 45us/step - loss: 0.4092 - acc: 0.7986 - val_loss: 0.4277 - val_acc: 0.7868\n",
      "Epoch 31/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.7984Epoch 00031: val_acc improved from 0.78680 to 0.78680, saving model to weights/weights-31-0.4275.hdf5\n",
      "133422/133422 [==============================] - 6s 43us/step - loss: 0.4093 - acc: 0.7985 - val_loss: 0.4275 - val_acc: 0.7868\n",
      "Epoch 32/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.7982Epoch 00032: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 52us/step - loss: 0.4086 - acc: 0.7983 - val_loss: 0.4398 - val_acc: 0.7822\n",
      "Epoch 33/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.7995Epoch 00033: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.4078 - acc: 0.7995 - val_loss: 0.4386 - val_acc: 0.7813\n",
      "Epoch 34/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8002Epoch 00034: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 51us/step - loss: 0.4066 - acc: 0.8001 - val_loss: 0.4420 - val_acc: 0.7738\n",
      "Epoch 35/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.7999Epoch 00035: val_acc did not improve\n",
      "133422/133422 [==============================] - 6s 44us/step - loss: 0.4062 - acc: 0.7999 - val_loss: 0.4481 - val_acc: 0.7705\n",
      "Epoch 36/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8003Epoch 00036: val_acc improved from 0.78680 to 0.78803, saving model to weights/weights-36-0.4243.hdf5\n",
      "133422/133422 [==============================] - 7s 53us/step - loss: 0.4053 - acc: 0.8004 - val_loss: 0.4243 - val_acc: 0.7880\n",
      "Epoch 37/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8003- ETA: 0s - loss: 0.4050 - acc: 0.800Epoch 00037: val_acc improved from 0.78803 to 0.78810, saving model to weights/weights-37-0.4246.hdf5\n",
      "133422/133422 [==============================] - 7s 55us/step - loss: 0.4052 - acc: 0.8002 - val_loss: 0.4246 - val_acc: 0.7881\n",
      "Epoch 38/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.7999Epoch 00038: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 53us/step - loss: 0.4060 - acc: 0.8000 - val_loss: 0.4314 - val_acc: 0.7844\n",
      "Epoch 39/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8002Epoch 00039: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 52us/step - loss: 0.4051 - acc: 0.8002 - val_loss: 0.4283 - val_acc: 0.7836\n",
      "Epoch 40/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.7979Epoch 00040: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 49us/step - loss: 0.4064 - acc: 0.7981 - val_loss: 0.4463 - val_acc: 0.7717\n",
      "Epoch 41/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.7996Epoch 00041: val_acc did not improve\n",
      "133422/133422 [==============================] - 8s 62us/step - loss: 0.4069 - acc: 0.7996 - val_loss: 0.4432 - val_acc: 0.7745\n",
      "Epoch 42/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8007- ETEpoch 00042: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 51us/step - loss: 0.4035 - acc: 0.8010 - val_loss: 0.4530 - val_acc: 0.7752\n",
      "Epoch 43/50\n",
      "132096/133422 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.7988Epoch 00043: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 51us/step - loss: 0.4079 - acc: 0.7988 - val_loss: 0.4430 - val_acc: 0.7735\n",
      "Epoch 44/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8009Epoch 00044: val_acc improved from 0.78810 to 0.78899, saving model to weights/weights-44-0.4226.hdf5\n",
      "133422/133422 [==============================] - 7s 52us/step - loss: 0.4032 - acc: 0.8008 - val_loss: 0.4226 - val_acc: 0.7890\n",
      "Epoch 45/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.7999Epoch 00045: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 51us/step - loss: 0.4040 - acc: 0.7999 - val_loss: 0.4411 - val_acc: 0.7813\n",
      "Epoch 46/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8012Epoch 00046: val_acc did not improve\n",
      "133422/133422 [==============================] - 9s 64us/step - loss: 0.4026 - acc: 0.8012 - val_loss: 0.4270 - val_acc: 0.7873\n",
      "Epoch 47/50\n",
      "132608/133422 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8011Epoch 00047: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 52us/step - loss: 0.4027 - acc: 0.8011 - val_loss: 0.4238 - val_acc: 0.7864\n",
      "Epoch 48/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8031Epoch 00048: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 54us/step - loss: 0.3996 - acc: 0.8032 - val_loss: 0.4237 - val_acc: 0.7872\n",
      "Epoch 49/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8003Epoch 00049: val_acc did not improve\n",
      "133422/133422 [==============================] - 8s 61us/step - loss: 0.4029 - acc: 0.8003 - val_loss: 0.4206 - val_acc: 0.7889\n",
      "Epoch 50/50\n",
      "133120/133422 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8025Epoch 00050: val_acc did not improve\n",
      "133422/133422 [==============================] - 7s 50us/step - loss: 0.4005 - acc: 0.8025 - val_loss: 0.4253 - val_acc: 0.7876\n"
     ]
    }
   ],
   "source": [
    "# Train network\n",
    "neuralnet.fit(image_features, object_matrix, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:02.176399\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:03.243434\n",
      "creating index...\n",
      "index created!\n",
      "Image features (243024, 1024)\n",
      "Question features (243024, 32)\n",
      "Dictionary size 14178\n"
     ]
    }
   ],
   "source": [
    "question_type = 'yes/no'\n",
    "# Load validation set and evaluate prediction on it\n",
    "pt= PrepareData(path_images='data_vqa_feat', # Path to image features \n",
    "                        subset='val2014', # Desired subset: either train2014 or val2014\n",
    "                        taskType=taskType, # 'OpenEnded', 'MultipleChoice', 'all'\n",
    "                        cut_data=data_amount, # Percentage of data to use, 1 = All values, above 1 = 10 samples for debugging\n",
    "                        output_path='data', # Path where we want to output temporary data\n",
    "                        pad_length=32)\n",
    "pt.loadDictionary('data/dictionary_all_types.pkl') # Use same dictionary as in training\n",
    "image_features, questions, _, annotations = pt.load_data()\n",
    "print(\"Image features\", image_features.shape)\n",
    "print(\"Question features\", questions.shape)\n",
    "print(\"Dictionary size\", pt.dic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243024,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check prediction accuracy of answer-type classifier\n",
    "y = np.array([2 if ann['answer_type'] == 'number' else 1 if ann['answer_type'] == 'yes/no' else 0 for ann in annotations])\n",
    "\n",
    "# Predict\n",
    "#pred = classifier.predict(questions)\n",
    "pred = neuralnetQuestionType.predict(questions, 'weights/weights-18-0.0089.hdf5')\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer type classification accuracy: 0.990206728554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# TODO: can probably still improve this accuracy\n",
    "print('Answer type classification accuracy:', accuracy_score(pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92090, 1024)\n",
      "(92090, 32)\n",
      "(92090,)\n"
     ]
    }
   ],
   "source": [
    "# Filter questions accordingly to their predicted type\n",
    "question_type_idx = 2 if question_type == 'number' else 1 if question_type == 'yes/no' else 0\n",
    "image_features = image_features[pred == question_type_idx, :]\n",
    "questions = questions[pred == question_type_idx, :]\n",
    "original_questions = np.array(pt._original_questions)[pred == question_type_idx]\n",
    "print(image_features.shape)\n",
    "print(questions.shape)\n",
    "print(original_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider three thresholds\n",
    "eo = ExtractObjects(cut_data=data_amount, output_fileName='objects_val.txt', subset='val2014', threshold=25)\n",
    "object_matrix1 = eo.onehotvector(original_questions)\n",
    "eo = ExtractObjects(cut_data=data_amount, output_fileName='objects_val.txt', subset='val2014', threshold=50)\n",
    "object_matrix2 = eo.onehotvector(original_questions)\n",
    "eo = ExtractObjects(cut_data=data_amount, output_fileName='objects_val.txt', subset='val2014', threshold=75)\n",
    "object_matrix3 = eo.onehotvector(original_questions)\n",
    "\n",
    "object_matrix = np.concatenate([object_matrix1, object_matrix2, object_matrix3], axis = 1)\n",
    "print(object_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92090, 3)\n"
     ]
    }
   ],
   "source": [
    "# Test prediction on validation set\n",
    "# pred = neuralnet.predict(image_features, questions, 'weights/weights-44-0.4226.hdf5')\n",
    "pred = neuralnet.predict_current_state(image_features, questions)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:02.359158\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "computing accuracy\n",
      "Finshed Percent: [####################] 99% Done computing accuracy\n",
      "\n",
      "\n",
      "Overall Accuracy is: 74.84\n",
      "\n",
      "Per Question Type Accuracy is the following:\n",
      "is the : 72.66\n",
      "is this an : 75.10\n",
      "are there : 80.00\n",
      "is it : 77.09\n",
      "is this : 74.65\n",
      "is there a : 85.36\n",
      "is the person : 75.09\n",
      "is this a : 77.13\n",
      "do : 69.75\n",
      "are the : 72.02\n",
      "are : 72.90\n",
      "does this : 75.08\n",
      "has : 76.46\n",
      "is the man : 75.31\n",
      "are they : 74.64\n",
      "is : 76.32\n",
      "is this person : 71.16\n",
      "are these : 73.15\n",
      "is there : 80.02\n",
      "do you : 75.62\n",
      "none of the above : 68.61\n",
      "does the : 73.37\n",
      "are there any : 72.74\n",
      "is he : 75.50\n",
      "is the woman : 73.87\n",
      "was : 73.36\n",
      "could : 85.54\n",
      "can you : 71.99\n",
      "is that a : 75.54\n",
      "how : 3.60\n",
      "what : 0.00\n",
      "how many : 0.00\n",
      "\n",
      "\n",
      "Per Answer Type Accuracy is the following:\n",
      "yes/no : 76.03\n",
      "other : 8.04\n",
      "number : 7.04\n",
      "\n",
      "\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:02.705804\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "computing accuracy\n",
      "Finshed Percent: [####################] 99% Done computing accuracy\n",
      "\n",
      "\n",
      "Overall Accuracy is: 74.84\n",
      "\n",
      "Per Question Type Accuracy is the following:\n",
      "is the : 72.66\n",
      "is this an : 75.10\n",
      "are there : 80.00\n",
      "is it : 77.09\n",
      "is this : 74.65\n",
      "is there a : 85.36\n",
      "is the person : 75.09\n",
      "is this a : 77.13\n",
      "do : 69.75\n",
      "are the : 72.02\n",
      "are : 72.90\n",
      "does this : 75.08\n",
      "has : 76.46\n",
      "is the man : 75.31\n",
      "are they : 74.64\n",
      "is : 76.32\n",
      "is this person : 71.16\n",
      "are these : 73.15\n",
      "is there : 80.02\n",
      "do you : 75.62\n",
      "none of the above : 68.61\n",
      "does the : 73.37\n",
      "are there any : 72.74\n",
      "is he : 75.50\n",
      "is the woman : 73.87\n",
      "was : 73.36\n",
      "could : 85.54\n",
      "can you : 71.99\n",
      "is that a : 75.54\n",
      "how : 3.60\n",
      "what : 0.00\n",
      "how many : 0.00\n",
      "\n",
      "\n",
      "Per Answer Type Accuracy is the following:\n",
      "yes/no : 76.03\n",
      "other : 8.04\n",
      "number : 7.04\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from EvaluateModel import ProduceResult\n",
    "model_evaluator = ProduceResult(p._int_to_answer, p._answer_to_int, dataSubType='val2014')\n",
    "answers = model_evaluator.produce_results(pred, original_questions)\n",
    "model_evaluator.evaluate(taskType=taskType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
